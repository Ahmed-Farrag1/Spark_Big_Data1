# Spark_Big_Data1
In this repository I am sharing practical examples about using Spark in Analyzing Massive datasets 
Apache Spark
Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Scala, Java, Python, and R, and an optimized engine that supports general computation graphs for data analysis. It also supports a rich set of higher-level tools including Spark SQL for SQL and DataFrames, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for stream processing.
https://spark.apache.org/
       
Online Documentation
You can find the latest Spark documentation, including a programming guide, on the project web page. This README file only contains basic setup instructions.

Taming Big Data with Apache Spark and Python hands on!
https://www.udemy.com/course/taming-big-data-with-apache-spark-hands-on/
This course was my resource for learning  Analyzing massive data on distributed systems and being able to apply what I could learn with hands on practice to Apache spark ,python , Amazon cloud services.
I could learn to 
•	Use DataFrames and Structured Streaming in Spark 3
•	Use the MLLib machine learning library to answer common data mining questions
•	Understand how Spark Streaming lets your process continuous streams of data in real time
•	Frame big data analysis problems as Spark problems
•	Use Amazon's Elastic MapReduce service to run your job on a cluster with Hadoop YARN
•	Install and run Apache Spark on a desktop computer or on a cluster
•	Use Spark's Resilient Distributed Datasets to process and analyze large data sets across many CPU's
•	Implement iterative algorithms such as breadth-first-search using Spark
•	Understand how Spark SQL lets you work with structured data
•	Tune and troubleshoot large jobs running on a cluster
•	Share information between nodes on a Spark cluster using broadcast variables and accumulators
•	Understand how the GraphX library helps with network analysis problems



